# Story 4.8: DuckDB Migration - Full Database Architecture Transformation

## ⚠️ DEFERRED UNTIL POST-PRODUCTION (100+ FLIGHT HOURS)

**PREREQUISITE**: This story requires real flight data for meaningful analytics. It should NOT be started until:
1. Story 4.7 (Hardware Integration) is complete
2. Story 3.4 (Field Testing) has generated 100+ hours of flight data
3. Production system is operational with SQLite
4. Analytics requirements are validated from real operations

## Story Summary

**As a** system architect,
**I want** to migrate from SQLite to DuckDB as our primary database engine,
**so that** we can leverage advanced analytical capabilities for real-time signal processing, gradient calculations, and performance analytics both during flight and post-mission.

**Implementation Note**: This story will be implemented in a forked codebase to maintain stability of the SQLite version while developing the DuckDB integration in parallel.

## Background & Motivation

### Why DuckDB Over SQLite

Based on comprehensive analysis of our stories and requirements:

1. **Real-Time Analytics** (In-Flight):
   - Native percentile functions for noise floor calculation (PRD FR6)
   - Window functions for gradient calculation (Story 3.2)
   - Regression analysis for predictive homing
   - Pattern detection for beacon identification

2. **Post-Mission Analytics** (Story 3.5):
   - 10-20x faster aggregations for performance validation
   - Statistical functions for 70% improvement verification
   - Time-series analysis for telemetry data
   - Correlation analysis for environmental factors

3. **Technical Advantages**:
   - Columnar storage for efficient analytics
   - Vectorized query execution (SIMD operations)
   - Native support for complex SQL analytics
   - Better compression for telemetry data

## Current State Analysis

### Files Requiring Modification

| Component | File | Lines | Impact Level |
|-----------|------|-------|--------------|
| Core Database | `src/backend/models/database.py` | 616 lines | HIGH - Complete refactor |
| State Machine | `src/backend/services/state_machine.py` | 50+ | MEDIUM - DB integration |
| Configuration | `src/backend/core/config.py` | 65 | LOW - Path updates |
| Test Logger | `src/backend/utils/test_logger.py` | 413 lines | HIGH - Schema changes |
| Unit Tests | `tests/backend/unit/test_database_models.py` | 398+ | HIGH - Mock updates |
| Integration Tests | `tests/backend/integration/conftest.py` | 173+ | MEDIUM - Async patterns |
| API Routes | `src/backend/api/routes/testing.py` | 22+ | LOW - Connection updates |

### Breaking Changes Identified

1. **AUTOINCREMENT** → Sequence-based IDs
2. **sqlite3 module** → duckdb module
3. **Connection patterns** → DuckDB connection syntax
4. **Transaction semantics** → Different isolation levels
5. **PRAGMA statements** → DuckDB equivalents
6. **Boolean type** → BOOL vs BOOLEAN
7. **Timestamp handling** → TIMESTAMPTZ support

## Forking Strategy

### Branch Management

1. **Main Branch**: Continue with SQLite for production stability
2. **Feature Branch**: `feature/duckdb-migration` for DuckDB development
3. **Testing**: Parallel testing environments for both versions
4. **Merge Strategy**: Once DuckDB version is validated, merge back to main

### Development Approach

```bash
# Create fork for DuckDB development
git checkout -b feature/duckdb-migration
git push origin feature/duckdb-migration

# Maintain two environments
SQLite Version: Production-ready, stable
DuckDB Version: Advanced features, testing
```

## Implementation Sprints

### Sprint 1: Foundation & Compatibility Layer (2 days)

#### Objectives
- Install DuckDB and create compatibility layer
- Maintain backward compatibility during transition
- Set up dual-database testing environment

#### Tasks

1. **Install DuckDB Dependencies**
   ```python
   # pyproject.toml additions
   duckdb = "^1.1.3"
   duckdb-engine = "^0.13.0"  # SQLAlchemy adapter
   ```

2. **Create Database Abstraction Layer**
   ```python
   # src/backend/models/db_adapter.py
   class DatabaseAdapter(Protocol):
       def connect(self, path: str) -> Connection: ...
       def execute(self, query: str, params: tuple) -> Cursor: ...
       def commit(self) -> None: ...
       def close(self) -> None: ...

   class SQLiteAdapter(DatabaseAdapter):
       # Existing SQLite implementation

   class DuckDBAdapter(DatabaseAdapter):
       # New DuckDB implementation
   ```

3. **Environment-Based Database Selection**
   ```python
   # src/backend/core/config.py
   DATABASE_ENGINE = os.getenv("DATABASE_ENGINE", "duckdb")  # Changed default
   DB_PATH = "data/pisad.duckdb"  # New extension
   ```

4. **Create Migration Scripts**
   ```python
   # scripts/migrate_to_duckdb.py
   def migrate_sqlite_to_duckdb(sqlite_path: str, duckdb_path: str):
       # Export SQLite data
       # Transform schema
       # Import to DuckDB
   ```

#### Acceptance Criteria
- [ ] DuckDB installed and importable
- [ ] Abstraction layer passes all unit tests
- [ ] Migration script successfully converts test database
- [ ] Both databases can run in parallel

### Sprint 2: Schema Transformation (2 days)

#### Objectives
- Convert all table schemas to DuckDB format
- Implement sequences for auto-incrementing IDs
- Add analytical columns and indexes

#### Tasks

1. **Transform config_profiles Table**
   ```sql
   -- DuckDB version with sequences
   CREATE SEQUENCE seq_config_profiles START 1;

   CREATE TABLE config_profiles (
       id VARCHAR DEFAULT 'prof_' || nextval('seq_config_profiles')::VARCHAR PRIMARY KEY,
       name VARCHAR UNIQUE NOT NULL,
       description VARCHAR,
       sdr_config JSON,  -- Native JSON type
       signal_config JSON,
       homing_config JSON,
       is_default BOOLEAN DEFAULT FALSE,
       created_at TIMESTAMPTZ DEFAULT NOW(),
       updated_at TIMESTAMPTZ DEFAULT NOW()
   );
   ```

2. **Transform state_history Table**
   ```sql
   CREATE SEQUENCE seq_state_history START 1;

   CREATE TABLE state_history (
       id BIGINT DEFAULT nextval('seq_state_history') PRIMARY KEY,
       from_state VARCHAR NOT NULL,
       to_state VARCHAR NOT NULL,
       timestamp TIMESTAMPTZ NOT NULL,
       reason VARCHAR,
       operator_id VARCHAR,
       action_duration_ms INTEGER,
       created_at TIMESTAMPTZ DEFAULT NOW(),
       -- New analytical columns
       session_id UUID,
       signal_strength DOUBLE,
       location GEOMETRY  -- Spatial support
   );

   -- Analytical indexes
   CREATE INDEX idx_state_history_timestamp ON state_history(timestamp);
   CREATE INDEX idx_state_history_session ON state_history(session_id);
   ```

3. **Create New Analytical Tables**
   ```sql
   -- Real-time RSSI circular buffer
   CREATE TABLE rssi_buffer (
       timestamp TIMESTAMPTZ PRIMARY KEY,
       rssi DOUBLE NOT NULL,
       noise_floor DOUBLE,
       snr DOUBLE GENERATED ALWAYS AS (rssi - noise_floor) STORED,
       location GEOMETRY,
       gradient DOUBLE  -- Computed in real-time
   );

   -- Telemetry time-series
   CREATE TABLE telemetry (
       timestamp TIMESTAMPTZ,
       metric_name VARCHAR,
       metric_value DOUBLE,
       metadata JSON
   ) PARTITION BY RANGE (timestamp);  -- Partitioned for performance
   ```

4. **Update Database Models**
   ```python
   # src/backend/models/database.py
   class ConfigProfileDB:
       def _init_database(self) -> None:
           conn = duckdb.connect(str(self.db_path))
           # New schema creation
           conn.execute(DUCKDB_SCHEMA)
   ```

#### Acceptance Criteria
- [ ] All tables created successfully in DuckDB
- [ ] Sequences working for auto-increment
- [ ] JSON columns properly typed
- [ ] Spatial/geometry support enabled
- [ ] Indexes created for performance

### Sprint 3: Query Migration & Optimization (3 days)

#### Objectives
- Convert all SQL queries to DuckDB syntax
- Implement analytical queries for real-time processing
- Optimize for vectorized execution

#### Tasks

1. **Update ConfigProfileDB Queries**
   ```python
   # Before (SQLite)
   cursor.execute("""
       INSERT INTO config_profiles (id, name, sdr_config)
       VALUES (?, ?, ?)
   """, (id, name, json.dumps(sdr_config)))

   # After (DuckDB)
   conn.execute("""
       INSERT INTO config_profiles (name, sdr_config)
       VALUES ($1, $2::JSON)
       RETURNING id
   """, [name, sdr_config])  # Native JSON, auto ID
   ```

2. **Implement Real-Time Analytics Queries**
   ```python
   # src/backend/services/signal_analytics.py
   class SignalAnalytics:
       def calculate_noise_floor(self, window_seconds: int = 10) -> float:
           """Calculate noise floor using 10th percentile (PRD FR6)"""
           return self.conn.execute("""
               SELECT percentile_cont(0.1) WITHIN GROUP (ORDER BY rssi)
               FROM rssi_buffer
               WHERE timestamp > NOW() - INTERVAL ? SECOND
           """, [window_seconds]).fetchone()[0]

       def calculate_gradient(self) -> tuple[float, float]:
           """Real-time gradient for homing"""
           return self.conn.execute("""
               SELECT
                   regr_slope(rssi, EXTRACT(EPOCH FROM timestamp)) as gradient,
                   regr_r2(rssi, EXTRACT(EPOCH FROM timestamp)) as confidence
               FROM (
                   SELECT timestamp, rssi
                   FROM rssi_buffer
                   WHERE timestamp > NOW() - INTERVAL '2 SECOND'
                   ORDER BY timestamp DESC
                   LIMIT 20
               ) recent_readings
           """).fetchone()
   ```

3. **Optimize State Machine Queries**
   ```python
   # State transition with analytics
   def record_transition(self, from_state, to_state, reason, signal_data):
       self.conn.execute("""
           INSERT INTO state_history
           (from_state, to_state, timestamp, reason, signal_strength, location)
           VALUES ($1, $2, NOW(), $3, $4, ST_Point($5, $6))
       """, [from_state, to_state, reason,
             signal_data['rssi'],
             signal_data['lat'], signal_data['lon']])
   ```

4. **Implement Window Functions for Analysis**
   ```python
   def get_signal_pattern(self) -> dict:
       """Detect beacon patterns using window functions"""
       return self.conn.execute("""
           WITH signal_analysis AS (
               SELECT
                   timestamp,
                   rssi,
                   rssi - LAG(rssi) OVER w as delta,
                   AVG(rssi) OVER w as moving_avg,
                   STDDEV(rssi) OVER w as volatility
               FROM rssi_buffer
               WHERE timestamp > NOW() - INTERVAL '30 SECOND'
               WINDOW w AS (ORDER BY timestamp ROWS BETWEEN 5 PRECEDING AND CURRENT ROW)
           )
           SELECT
               COUNT(CASE WHEN delta > 10 THEN 1 END) as pulse_count,
               AVG(volatility) as signal_stability,
               MAX(rssi) - MIN(rssi) as signal_range
           FROM signal_analysis
       """).fetchdf()  # Return as DataFrame
   ```

#### Acceptance Criteria
- [ ] All CRUD operations working with DuckDB
- [ ] Real-time analytics queries implemented
- [ ] Window functions for gradient calculation
- [ ] Percentile functions for noise floor
- [ ] Performance benchmarks show improvement

### Sprint 4: Connection Management & Performance (2 days)

#### Objectives
- Implement connection pooling for DuckDB
- Optimize for concurrent read/write patterns
- Set up in-memory analytics tables

#### Tasks

1. **Implement Connection Pool**
   ```python
   # src/backend/models/connection_pool.py
   class DuckDBPool:
       def __init__(self, path: str, max_connections: int = 5):
           self.path = path
           self.pool = Queue(maxsize=max_connections)
           self._initialize_pool()

       def _initialize_pool(self):
           for _ in range(self.pool.maxsize):
               conn = duckdb.connect(self.path)
               # Configure connection
               conn.execute("SET memory_limit='500MB'")
               conn.execute("SET threads=2")  # For Pi 5
               self.pool.put(conn)

       @contextmanager
       def get_connection(self):
           conn = self.pool.get()
           try:
               yield conn
           finally:
               self.pool.put(conn)
   ```

2. **Implement Hybrid Storage Strategy**
   ```python
   # src/backend/services/database_service.py
   class HybridDatabase:
       def __init__(self):
           # Persistent storage
           self.persistent = duckdb.connect('data/pisad.duckdb')

           # In-memory analytics (circular buffer)
           self.analytics = duckdb.connect(':memory:')
           self.analytics.execute("SET memory_limit='100MB'")
           self._setup_analytics_tables()

       def _setup_analytics_tables(self):
           """Create in-memory tables for real-time analytics"""
           self.analytics.execute("""
               CREATE TABLE rssi_realtime AS
               SELECT * FROM read_csv_auto('/dev/null')
               WHERE 1=0  -- Empty table with schema
           """)

       def write_rssi(self, reading: dict):
           """Write to both persistent and analytics"""
           # Persistent record
           self.persistent.execute(
               "INSERT INTO rssi_readings VALUES ($1, $2, $3)",
               [reading['timestamp'], reading['rssi'], reading['noise_floor']]
           )

           # Analytics buffer (keep last 1000 readings)
           self.analytics.execute("""
               DELETE FROM rssi_realtime
               WHERE timestamp < (
                   SELECT MIN(timestamp) FROM (
                       SELECT timestamp FROM rssi_realtime
                       ORDER BY timestamp DESC LIMIT 999
                   )
               )
           """)
           self.analytics.execute(
               "INSERT INTO rssi_realtime VALUES ($1, $2, $3)",
               [reading['timestamp'], reading['rssi'], reading['noise_floor']]
           )
   ```

3. **Optimize Write Performance**
   ```python
   # Batch writes for high-frequency data
   class BatchWriter:
       def __init__(self, conn: duckdb.Connection, table: str, batch_size: int = 100):
           self.conn = conn
           self.table = table
           self.batch = []
           self.batch_size = batch_size

       async def write(self, record: dict):
           self.batch.append(record)
           if len(self.batch) >= self.batch_size:
               await self.flush()

       async def flush(self):
           if self.batch:
               df = pd.DataFrame(self.batch)
               self.conn.execute(f"INSERT INTO {self.table} SELECT * FROM df")
               self.batch.clear()
   ```

#### Acceptance Criteria
- [ ] Connection pooling implemented and tested
- [ ] Concurrent read/write patterns optimized
- [ ] In-memory analytics tables working
- [ ] Batch writing for high-frequency data
- [ ] Memory usage within Pi 5 constraints

### Sprint 5: Testing & Migration (2 days)

#### Objectives
- Update all test files for DuckDB
- Migrate existing data
- Validate performance improvements

#### Tasks

1. **Update Test Fixtures**
   ```python
   # tests/conftest.py
   @pytest.fixture
   async def db_connection():
       """DuckDB test fixture"""
       conn = duckdb.connect(':memory:')
       # Create test schema
       conn.execute(TEST_SCHEMA)
       yield conn
       conn.close()
   ```

2. **Update Unit Tests**
   ```python
   # tests/backend/unit/test_database_models.py
   def test_config_profile_creation():
       conn = duckdb.connect(':memory:')
       # Test with DuckDB syntax
       conn.execute("""
           INSERT INTO config_profiles (name, sdr_config)
           VALUES ('test', '{"frequency": 2437000000}'::JSON)
       """)
       result = conn.execute("SELECT * FROM config_profiles").fetchdf()
       assert len(result) == 1
   ```

3. **Create Migration Validation Tests**
   ```python
   def test_data_migration():
       # Load SQLite data
       sqlite_conn = sqlite3.connect('test_old.db')
       old_data = pd.read_sql("SELECT * FROM config_profiles", sqlite_conn)

       # Load DuckDB data
       duckdb_conn = duckdb.connect('test_new.duckdb')
       new_data = duckdb_conn.execute("SELECT * FROM config_profiles").fetchdf()

       # Validate migration
       pd.testing.assert_frame_equal(old_data, new_data)
   ```

4. **Performance Benchmarks**
   ```python
   # tests/benchmarks/test_analytics_performance.py
   def benchmark_noise_floor_calculation():
       # Generate test data
       conn = duckdb.connect(':memory:')
       conn.execute("""
           CREATE TABLE rssi_test AS
           SELECT
               NOW() - INTERVAL (i || ' seconds') as timestamp,
               -70 + RANDOM() * 20 as rssi
           FROM generate_series(1, 10000) as t(i)
       """)

       # Benchmark percentile calculation
       start = time.time()
       for _ in range(100):
           conn.execute("""
               SELECT percentile_cont(0.1) WITHIN GROUP (ORDER BY rssi)
               FROM rssi_test
               WHERE timestamp > NOW() - INTERVAL '10 SECOND'
           """).fetchone()
       duration = time.time() - start

       assert duration < 1.0  # Should complete 100 queries in < 1 second
   ```

#### Acceptance Criteria
- [ ] All unit tests passing with DuckDB
- [ ] Integration tests updated and passing
- [ ] Data migration validated
- [ ] Performance benchmarks show improvement
- [ ] No data loss during migration

## Backend Reality Update - Sentinel (2025-01-15)

### Analytics Backend Already Exists

**Current Implementation Status:**
- ✅ **Analytics Service** (`src/backend/services/performance_analytics.py`) - Complete
- ✅ **Report Generator** (`src/backend/services/report_generator.py`) - Implemented
- ✅ **Mission Replay** (`src/backend/services/mission_replay_service.py`) - Working
- ✅ **Analytics API** (`src/backend/api/routes/analytics.py`) - All endpoints ready
- ✅ **SQLAlchemy ORM** - Already using async with aiosqlite

**What DuckDB Migration Actually Means:**
1. **Replace SQLite** with DuckDB for better analytics performance
2. **Keep existing service layer** - Only change database engine
3. **Update SQLAlchemy dialect** from `sqlite+aiosqlite` to `duckdb`
4. **Leverage DuckDB features** for existing analytics:
   - Percentile functions for noise floor (already implemented, just slower in SQLite)
   - Window functions for gradient analysis (current implementation exists)
   - Time-series analysis for telemetry (service exists, needs optimization)

**Implementation Simplification:**
- This is a **database engine swap**, not a full rewrite
- All services and APIs remain unchanged
- Only database connection and query optimization needed
- Expected effort: 2-3 days instead of 2 weeks

**Current Analytics Capabilities (SQLite):**
- Signal statistics calculation
- Performance metrics aggregation
- Mission replay with time control
- Report generation (PDF/JSON/CSV)
- Recommendations engine

**After DuckDB Migration:**
- Same capabilities, 10-20x faster
- Better memory efficiency
- Advanced statistical functions
- Parallel query execution

### Sprint 6: Deployment & Monitoring (1 day)

#### Objectives
- Deploy to development environment
- Monitor performance and memory usage
- Document configuration and operations

#### Tasks

1. **Update Deployment Configuration**
   ```yaml
   # deployment/docker-compose.yml
   environment:
     DATABASE_ENGINE: duckdb
     DUCKDB_MEMORY_LIMIT: 500MB
     DUCKDB_THREADS: 2
   ```

2. **Create Operational Scripts**
   ```bash
   # scripts/db_maintenance.sh
   #!/bin/bash
   # Vacuum and analyze DuckDB
   duckdb data/pisad.duckdb "VACUUM; ANALYZE;"

   # Export for backup
   duckdb data/pisad.duckdb "EXPORT DATABASE 'backup' (FORMAT PARQUET);"
   ```

3. **Implement Monitoring**
   ```python
   # src/backend/services/db_monitor.py
   class DatabaseMonitor:
       def get_stats(self) -> dict:
           return self.conn.execute("""
               SELECT
                   current_setting('memory_limit') as memory_limit,
                   current_setting('threads') as threads,
                   (SELECT COUNT(*) FROM rssi_buffer) as buffer_size,
                   (SELECT pg_size_pretty(SUM(bytes)) FROM duckdb_databases()) as db_size
           """).fetchdf().to_dict()
   ```

#### Acceptance Criteria
- [ ] Deployed to development environment
- [ ] Memory usage within limits
- [ ] Backup/restore procedures working
- [ ] Monitoring dashboard showing metrics
- [ ] Documentation complete

## Dependencies

### Technical Dependencies
- DuckDB Python package (1.1.3+)
- Pandas for DataFrame operations
- pyarrow for Parquet support (optional)

### Story Dependencies
- Builds on Story 4.3 (Hardware Integration)
- Enables Story 3.5 (Performance Analytics)
- Enhances Story 3.2 (Gradient Homing)

## Success Metrics

1. **Performance Improvements**:
   - Noise floor calculation: <10ms (from ~50ms)
   - Gradient calculation: <20ms (from ~100ms)
   - Analytics queries: 5-20x faster
   - Mission replay: <1 second for 1-hour mission

2. **Operational Metrics**:
   - Memory usage: <500MB during flight
   - Database size: Similar to SQLite (~10MB/hour)
   - Write performance: >100 writes/second
   - Query concurrency: 5+ simultaneous queries

3. **Feature Enablement**:
   - Real-time percentile calculations working
   - Window functions for gradient analysis
   - Statistical functions for validation
   - Spatial queries for search optimization

## Risk Mitigation

### Identified Risks

1. **Write Performance Degradation**
   - Mitigation: Batch writing, in-memory buffers

2. **Memory Usage on Pi 5**
   - Mitigation: Strict memory limits, circular buffers

3. **Migration Data Loss**
   - Mitigation: Backup before migration, validation tests

4. **Learning Curve**
   - Mitigation: Comprehensive documentation, examples

## Documentation Requirements

1. **Migration Guide**: Step-by-step migration process
2. **Query Cookbook**: Common DuckDB queries for PISAD
3. **Performance Tuning**: Configuration for Pi 5
4. **Operational Procedures**: Backup, restore, maintenance

## Definition of Done

- [ ] All SQLite references removed from codebase
- [ ] DuckDB fully integrated and operational
- [ ] All tests passing with >80% coverage
- [ ] Performance benchmarks meet targets
- [ ] Documentation complete
- [ ] Deployed to development environment
- [ ] Team trained on DuckDB operations
- [ ] Migration rollback plan tested

## Notes

This migration represents a significant architectural change that will:
1. Enable real-time analytics during flight
2. Improve post-mission analysis capabilities
3. Support advanced features like pattern detection
4. Provide foundation for future ML integration

The sprintd approach ensures we maintain system stability while progressively introducing DuckDB's advanced capabilities.

## Change Log

| Date | Version | Changes | Author |
|------|---------|---------|--------|
| 2025-08-15 | 1.0 | Initial story creation with comprehensive migration plan | Claude (Assistant) |
